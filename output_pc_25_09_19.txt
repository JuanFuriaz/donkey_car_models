(donkey) root@beha-XPS-13-9360:/home/mbeha/donkey_car/anaconda/donkeycar/car# python manage.py train --tub data/tub_1_19-08-24/ --model ./models/test_model_manuel_pc_25_09_19.h5
using donkey v3.1.0 ...
loading config file: /home/mbeha/donkey_car/anaconda/donkeycar/car/config.py
loading personal config over-rides

config loaded
2019-09-25 17:23:54.089156: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-09-25 17:23:54.092455: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904265000 Hz
2019-09-25 17:23:54.092614: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55e72ba048e0 executing computations on platform Host. Devices:
2019-09-25 17:23:54.092645: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3
OMP: Info #156: KMP_AFFINITY: 4 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 
OMP: Info #250: KMP_AFFINITY: pid 3240 tid 3240 thread 0 bound to OS proc set 0
2019-09-25 17:23:54.092955: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
"get_model_by_type" model Type is: linear
WARNING:tensorflow:From /home/mbeha/donkey_car/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/mbeha/donkey_car/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/mbeha/donkey_car/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
training with model type <class 'donkeycar.parts.keras.KerasLinear'>
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
img_in (InputLayer)             (None, 120, 160, 3)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 58, 78, 24)   1824        img_in[0][0]                     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 58, 78, 24)   0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 27, 37, 32)   19232       dropout[0][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 27, 37, 32)   0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 12, 17, 64)   51264       dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 12, 17, 64)   0           conv2d_3[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 10, 15, 64)   36928       dropout_2[0][0]                  
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 10, 15, 64)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 8, 13, 64)    36928       dropout_3[0][0]                  
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 8, 13, 64)    0           conv2d_5[0][0]                   
__________________________________________________________________________________________________
flattened (Flatten)             (None, 6656)         0           dropout_4[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 100)          665700      flattened[0][0]                  
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 100)          0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 50)           5050        dropout_5[0][0]                  
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 50)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
n_outputs0 (Dense)              (None, 1)            51          dropout_6[0][0]                  
__________________________________________________________________________________________________
n_outputs1 (Dense)              (None, 1)            51          dropout_6[0][0]                  
==================================================================================================
Total params: 817,028
Trainable params: 817,028
Non-trainable params: 0
__________________________________________________________________________________________________
None
found 0 pickles writing json records and images in tub data/tub_1_19-08-24/
data/tub_1_19-08-24/
collating 2499 records ...
train: 1999, val: 500
total records: 2499
steps_per_epoch 15
WARNING:tensorflow:From /home/mbeha/donkey_car/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Epoch 1/100
OMP: Info #250: KMP_AFFINITY: pid 3240 tid 3256 thread 1 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 3240 tid 3298 thread 3 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 3240 tid 3299 thread 4 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 3240 tid 3297 thread 2 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 3240 tid 3257 thread 5 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 3240 tid 3301 thread 7 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 3240 tid 3300 thread 6 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 3240 tid 3302 thread 8 bound to OS proc set 0
3/3 [==============================] - 1s 283ms/step - loss: 0.5401 - n_outputs0_loss: 0.5350 - n_outputs1_loss: 0.0051

Epoch 00001: val_loss improved from inf to 0.54008, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 11s 740ms/step - loss: 0.6582 - n_outputs0_loss: 0.6099 - n_outputs1_loss: 0.0483 - val_loss: 0.5401 - val_n_outputs0_loss: 0.5350 - val_n_outputs1_loss: 0.0051
Epoch 2/100
3/3 [==============================] - 0s 137ms/step - loss: 0.5346 - n_outputs0_loss: 0.5332 - n_outputs1_loss: 0.0014

Epoch 00002: val_loss improved from 0.54008 to 0.53460, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 9s 609ms/step - loss: 0.5714 - n_outputs0_loss: 0.5632 - n_outputs1_loss: 0.0082 - val_loss: 0.5346 - val_n_outputs0_loss: 0.5332 - val_n_outputs1_loss: 0.0014
Epoch 3/100
3/3 [==============================] - 0s 142ms/step - loss: 0.5335 - n_outputs0_loss: 0.5324 - n_outputs1_loss: 0.0011

Epoch 00003: val_loss improved from 0.53460 to 0.53352, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 10s 654ms/step - loss: 0.5622 - n_outputs0_loss: 0.5587 - n_outputs1_loss: 0.0034 - val_loss: 0.5335 - val_n_outputs0_loss: 0.5324 - val_n_outputs1_loss: 0.0011
Epoch 4/100
3/3 [==============================] - 0s 132ms/step - loss: 0.4677 - n_outputs0_loss: 0.4605 - n_outputs1_loss: 0.0072

Epoch 00004: val_loss improved from 0.53352 to 0.46773, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 10s 647ms/step - loss: 0.5370 - n_outputs0_loss: 0.5321 - n_outputs1_loss: 0.0049 - val_loss: 0.4677 - val_n_outputs0_loss: 0.4605 - val_n_outputs1_loss: 0.0072
Epoch 5/100
3/3 [==============================] - 0s 150ms/step - loss: 0.3408 - n_outputs0_loss: 0.3274 - n_outputs1_loss: 0.0134

Epoch 00005: val_loss improved from 0.46773 to 0.34081, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 10s 674ms/step - loss: 0.4363 - n_outputs0_loss: 0.4269 - n_outputs1_loss: 0.0094 - val_loss: 0.3408 - val_n_outputs0_loss: 0.3274 - val_n_outputs1_loss: 0.0134
Epoch 6/100
3/3 [==============================] - 0s 161ms/step - loss: 0.2805 - n_outputs0_loss: 0.2759 - n_outputs1_loss: 0.0047

Epoch 00006: val_loss improved from 0.34081 to 0.28053, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 10s 683ms/step - loss: 0.3524 - n_outputs0_loss: 0.3413 - n_outputs1_loss: 0.0110 - val_loss: 0.2805 - val_n_outputs0_loss: 0.2759 - val_n_outputs1_loss: 0.0047
Epoch 7/100
3/3 [==============================] - 0s 158ms/step - loss: 0.2467 - n_outputs0_loss: 0.2448 - n_outputs1_loss: 0.0019

Epoch 00007: val_loss improved from 0.28053 to 0.24667, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 10s 664ms/step - loss: 0.3032 - n_outputs0_loss: 0.2953 - n_outputs1_loss: 0.0078 - val_loss: 0.2467 - val_n_outputs0_loss: 0.2448 - val_n_outputs1_loss: 0.0019
Epoch 8/100
3/3 [==============================] - 0s 157ms/step - loss: 0.2084 - n_outputs0_loss: 0.2066 - n_outputs1_loss: 0.0018

Epoch 00008: val_loss improved from 0.24667 to 0.20840, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 10s 658ms/step - loss: 0.2574 - n_outputs0_loss: 0.2502 - n_outputs1_loss: 0.0072 - val_loss: 0.2084 - val_n_outputs0_loss: 0.2066 - val_n_outputs1_loss: 0.0018
Epoch 9/100
3/3 [==============================] - 0s 129ms/step - loss: 0.2373 - n_outputs0_loss: 0.2358 - n_outputs1_loss: 0.0015

Epoch 00009: val_loss did not improve from 0.20840
15/15 [==============================] - 9s 616ms/step - loss: 0.2399 - n_outputs0_loss: 0.2324 - n_outputs1_loss: 0.0076 - val_loss: 0.2373 - val_n_outputs0_loss: 0.2358 - val_n_outputs1_loss: 0.0015
Epoch 10/100
3/3 [==============================] - 0s 132ms/step - loss: 0.1789 - n_outputs0_loss: 0.1748 - n_outputs1_loss: 0.0041

Epoch 00010: val_loss improved from 0.20840 to 0.17890, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 9s 583ms/step - loss: 0.2232 - n_outputs0_loss: 0.2169 - n_outputs1_loss: 0.0063 - val_loss: 0.1789 - val_n_outputs0_loss: 0.1748 - val_n_outputs1_loss: 0.0041
Epoch 11/100
3/3 [==============================] - 0s 133ms/step - loss: 0.1844 - n_outputs0_loss: 0.1829 - n_outputs1_loss: 0.0014

Epoch 00011: val_loss did not improve from 0.17890
15/15 [==============================] - 9s 579ms/step - loss: 0.2058 - n_outputs0_loss: 0.2004 - n_outputs1_loss: 0.0054 - val_loss: 0.1844 - val_n_outputs0_loss: 0.1829 - val_n_outputs1_loss: 0.0014
Epoch 12/100
3/3 [==============================] - 0s 139ms/step - loss: 0.1633 - n_outputs0_loss: 0.1595 - n_outputs1_loss: 0.0038

Epoch 00012: val_loss improved from 0.17890 to 0.16328, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 9s 584ms/step - loss: 0.1970 - n_outputs0_loss: 0.1911 - n_outputs1_loss: 0.0059 - val_loss: 0.1633 - val_n_outputs0_loss: 0.1595 - val_n_outputs1_loss: 0.0038
Epoch 13/100
3/3 [==============================] - 0s 130ms/step - loss: 0.1490 - n_outputs0_loss: 0.1478 - n_outputs1_loss: 0.0013

Epoch 00013: val_loss improved from 0.16328 to 0.14904, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 9s 581ms/step - loss: 0.1785 - n_outputs0_loss: 0.1740 - n_outputs1_loss: 0.0045 - val_loss: 0.1490 - val_n_outputs0_loss: 0.1478 - val_n_outputs1_loss: 0.0013
Epoch 14/100
3/3 [==============================] - 0s 135ms/step - loss: 0.1384 - n_outputs0_loss: 0.1373 - n_outputs1_loss: 0.0011

Epoch 00014: val_loss improved from 0.14904 to 0.13841, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 9s 602ms/step - loss: 0.1688 - n_outputs0_loss: 0.1642 - n_outputs1_loss: 0.0047 - val_loss: 0.1384 - val_n_outputs0_loss: 0.1373 - val_n_outputs1_loss: 0.0011
Epoch 15/100
3/3 [==============================] - 0s 133ms/step - loss: 0.1527 - n_outputs0_loss: 0.1515 - n_outputs1_loss: 0.0012

Epoch 00015: val_loss did not improve from 0.13841
15/15 [==============================] - 9s 581ms/step - loss: 0.1640 - n_outputs0_loss: 0.1597 - n_outputs1_loss: 0.0044 - val_loss: 0.1527 - val_n_outputs0_loss: 0.1515 - val_n_outputs1_loss: 0.0012
Epoch 16/100
3/3 [==============================] - 0s 135ms/step - loss: 0.1592 - n_outputs0_loss: 0.1580 - n_outputs1_loss: 0.0011

Epoch 00016: val_loss did not improve from 0.13841
15/15 [==============================] - 9s 593ms/step - loss: 0.1690 - n_outputs0_loss: 0.1647 - n_outputs1_loss: 0.0043 - val_loss: 0.1592 - val_n_outputs0_loss: 0.1580 - val_n_outputs1_loss: 0.0011
Epoch 17/100
3/3 [==============================] - 0s 129ms/step - loss: 0.1343 - n_outputs0_loss: 0.1326 - n_outputs1_loss: 0.0018

Epoch 00017: val_loss improved from 0.13841 to 0.13434, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 9s 586ms/step - loss: 0.1635 - n_outputs0_loss: 0.1597 - n_outputs1_loss: 0.0038 - val_loss: 0.1343 - val_n_outputs0_loss: 0.1326 - val_n_outputs1_loss: 0.0018
Epoch 18/100
3/3 [==============================] - 0s 132ms/step - loss: 0.1658 - n_outputs0_loss: 0.1643 - n_outputs1_loss: 0.0015

Epoch 00018: val_loss did not improve from 0.13434
15/15 [==============================] - 9s 597ms/step - loss: 0.1635 - n_outputs0_loss: 0.1594 - n_outputs1_loss: 0.0041 - val_loss: 0.1658 - val_n_outputs0_loss: 0.1643 - val_n_outputs1_loss: 0.0015
Epoch 19/100
3/3 [==============================] - 0s 132ms/step - loss: 0.1515 - n_outputs0_loss: 0.1502 - n_outputs1_loss: 0.0013

Epoch 00019: val_loss did not improve from 0.13434
15/15 [==============================] - 9s 579ms/step - loss: 0.1578 - n_outputs0_loss: 0.1540 - n_outputs1_loss: 0.0039 - val_loss: 0.1515 - val_n_outputs0_loss: 0.1502 - val_n_outputs1_loss: 0.0013
Epoch 20/100
3/3 [==============================] - 0s 132ms/step - loss: 0.1451 - n_outputs0_loss: 0.1441 - n_outputs1_loss: 0.0011

Epoch 00020: val_loss did not improve from 0.13434
15/15 [==============================] - 9s 579ms/step - loss: 0.1618 - n_outputs0_loss: 0.1585 - n_outputs1_loss: 0.0033 - val_loss: 0.1451 - val_n_outputs0_loss: 0.1441 - val_n_outputs1_loss: 0.0011
Epoch 21/100
3/3 [==============================] - 0s 129ms/step - loss: 0.1458 - n_outputs0_loss: 0.1446 - n_outputs1_loss: 0.0011

Epoch 00021: val_loss did not improve from 0.13434
15/15 [==============================] - 9s 577ms/step - loss: 0.1519 - n_outputs0_loss: 0.1487 - n_outputs1_loss: 0.0032 - val_loss: 0.1458 - val_n_outputs0_loss: 0.1446 - val_n_outputs1_loss: 0.0011
Epoch 22/100
3/3 [==============================] - 0s 132ms/step - loss: 0.1341 - n_outputs0_loss: 0.1330 - n_outputs1_loss: 0.0011

Epoch 00022: val_loss improved from 0.13434 to 0.13410, saving model to ./models/test_model_manuel_pc_25_09_19.h5
15/15 [==============================] - 9s 580ms/step - loss: 0.1553 - n_outputs0_loss: 0.1519 - n_outputs1_loss: 0.0034 - val_loss: 0.1341 - val_n_outputs0_loss: 0.1330 - val_n_outputs1_loss: 0.0011
Epoch 00022: early stopping


----------- Best Eval Loss :0.134098 ---------
QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'
(donkey) root@beha-XPS-13-9360:/home/mbeha/donkey_car/anaconda/donkeycar/car# 
 
